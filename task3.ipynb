{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current soln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- write a function to interpret / underatand user response\n",
    "- given this users response: this is type of the output\n",
    "    - what is the actual value that I should use\n",
    "        - for example all dates in datetime format, etc\n",
    "- inputs:\n",
    "    - original question\n",
    "    - output data type were looking for \n",
    "    - output options\n",
    "    - user response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def interpret_response(original_question: str, output_data_type: str, output_options, user_response: str, mistaken_response=None):\n",
    "    if output_options is None:\n",
    "        output_options = \"Can be anything\"\n",
    "    else:\n",
    "        output_options = ', '.join(output_options)\n",
    "    \n",
    "    \n",
    "    # TODO - Add a check for mistaken_response and ask the user to provide the correct response if the response is incorrect.\n",
    "    # TODO - implement a conifidence score to check the LLM's confidence in the response. Set a threshold to ask for user confirmation\n",
    "    prompt = (\n",
    "        f\"Given the following question and user response, please extract the relevant value.\\n\\n\"\n",
    "        f\"Question: {original_question}\\n\"\n",
    "        f\"Expected data type: {output_data_type}\\n\"\n",
    "        f\"Possible options: {output_options}\\n\"\n",
    "        f\"User response: {user_response}\\n\\n\"\n",
    "        f\"Please provide the extracted value (if possible options are given, make sure that the value exists in the possible options or return 'Not applicable'):\"\n",
    "    )\n",
    "    \n",
    "    response = llm.predict(prompt).strip()\n",
    "    \n",
    "    if output_data_type == \"number\":\n",
    "        print(response)\n",
    "        try:\n",
    "            value = float(response)\n",
    "            return value\n",
    "        except ValueError:\n",
    "            return \"Invalid response: Unable to extract a valid number.\"\n",
    "    \n",
    "    elif output_data_type == \"string\":\n",
    "        return response\n",
    "    \n",
    "    elif output_data_type == \"list\":\n",
    "        for option in output_options:\n",
    "            if option.lower() in response.lower():\n",
    "                return option\n",
    "        return \"Invalid response: Expected one of the options.\"\n",
    "    \n",
    "    else:\n",
    "        return \"Invalid output data type specified.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "The interpreted value is: 7.0\n"
     ]
    }
   ],
   "source": [
    "original_question = \"Please provide your pain severity on a scale from 1 to 10.\"\n",
    "output_data_type = \"number\"\n",
    "output_options = [\"1 (mild)\", \"2\", \"3\", \"4\", \"5 (moderate)\", \"6\", \"7\", \"8\", \"9\", \"10 (severe)\"]\n",
    "user_response = \"I would say my pain is around 7\"\n",
    "\n",
    "interpreted_value = interpret_response(original_question, output_data_type, output_options, user_response)\n",
    "print(f\"The interpreted value is: {interpreted_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The interpreted value is: Bush\n"
     ]
    }
   ],
   "source": [
    "original_question = \"What is your last name?\"\n",
    "output_data_type = \"string\"\n",
    "output_options = None\n",
    "user_response = \"My name is Benjamin James Bush\"\n",
    "\n",
    "interpreted_value = interpret_response(original_question, output_data_type, output_options, user_response)\n",
    "print(f\"The interpreted value is: {interpreted_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7, 4\n",
      "The interpreted value is: Invalid response: Unable to extract a valid number.\n"
     ]
    }
   ],
   "source": [
    "original_question = \"Please provide your pain severity on a scale from 1 to 10.\"\n",
    "output_data_type = \"number\"\n",
    "output_options = None\n",
    "user_response = \"My pain is sometimes around a 7, however it is mainly a 4.\"\n",
    "\n",
    "interpreted_value = interpret_response(original_question, output_data_type, output_options, user_response)\n",
    "print(f\"The interpreted value is: {interpreted_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to continue reprompting until we have a usable response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The interpreted value is: Upper back\n"
     ]
    }
   ],
   "source": [
    "original_question = \"Where in your back does it hurt\"\n",
    "output_data_type = \"string\"\n",
    "output_options = ['lower back', 'middle back', 'upper back']\n",
    "user_response = \"it hurts in the upper part of the lower back.\"\n",
    "\n",
    "interpreted_value = interpret_response(original_question, output_data_type, output_options, user_response)\n",
    "print(f\"The interpreted value is: {interpreted_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we confirm our response with the user. we can use the confirmation step as context to prevent making the same mistake"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
